{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Taxi Trip Data Analysis - Modeling\n",
    "\n",
    "This notebook covers the model training, evaluation, and interpretation steps for the NYC Taxi Trip Data Analysis project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# Add the project root directory to the Python path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import project modules\n",
    "from src.config import RESULTS_DIR, MODELS_DIR\n",
    "from src.data.feature_selection import (\n",
    "    select_features_mutual_info, select_features_lasso,\n",
    "    compare_feature_selection_methods, get_common_features\n",
    ")\n",
    "from src.models.trainer import (\n",
    "    prepare_data_for_modeling, train_baseline_model, train_linear_regression, \n",
    "    train_ridge_regression, train_random_forest, train_gradient_boosting,\n",
    "    train_knn_regressor, train_svm_regressor, train_naive_bayes,\n",
    "    evaluate_model, get_feature_importance\n",
    ")\n",
    "from src.models.evaluator import (\n",
    "    plot_residuals, plot_prediction_vs_actual, plot_error_distribution,\n",
    "    plot_feature_importance, compare_models\n",
    ")\n",
    "from src.utils.helpers import save_metrics, timer_decorator\n",
    "\n",
    "# Set up plotting\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Processed Data\n",
    "\n",
    "Let's load the processed data from the preprocessing notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed data\n",
    "processed_dir = os.path.join(RESULTS_DIR, 'processed_data')\n",
    "taxi_type = 'yellow'\n",
    "\n",
    "# Check if processed data exists\n",
    "processed_file = os.path.join(processed_dir, f\"{taxi_type}_features.csv\")\n",
    "if os.path.exists(processed_file):\n",
    "    df = pd.read_csv(processed_file)\n",
    "    print(f\"Loaded processed data from {processed_file}\")\n",
    "else:\n",
    "    print(f\"Processed data file {processed_file} not found. Please run the preprocessing notebook first.\")\n",
    "    # If the file doesn't exist, we'll use the data from the main script output\n",
    "    # This is just a fallback in case the preprocessing notebook hasn't been run\n",
    "    from src.data.loader import load_taxi_data\n",
    "    from src.data.cleaner import clean_yellow_taxi_data\n",
    "    from src.data.feature_engineering import engineer_features\n",
    "    \n",
    "    print(\"Loading and processing data directly...\")\n",
    "    raw_df = load_taxi_data(taxi_type, ['2025-01', '2025-02'])\n",
    "    # Sample for faster processing\n",
    "    if len(raw_df) > 100000:\n",
    "        raw_df = raw_df.sample(n=100000, random_state=42)\n",
    "    clean_df = clean_yellow_taxi_data(raw_df)\n",
    "    df = engineer_features(clean_df, taxi_type)\n",
    "    print(f\"Processed data shape: {df.shape}\")\n",
    "\n",
    "# Load selected features if available\n",
    "selected_features_file = os.path.join(processed_dir, f\"{taxi_type}_selected_features.txt\")\n",
    "if os.path.exists(selected_features_file):\n",
    "    with open(selected_features_file, 'r') as f:\n",
    "        selected_features = [line.strip() for line in f.readlines()]\n",
    "    print(f\"Loaded {len(selected_features)} selected features from {selected_features_file}\")\n",
    "else:\n",
    "    selected_features = None\n",
    "    print(\"No selected features file found. Will select features during modeling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Data for Modeling\n",
    "\n",
    "Let's prepare the data for modeling by splitting it into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "target_col = 'trip_duration'\n",
    "if target_col not in df.columns:\n",
    "    raise ValueError(f\"Target column '{target_col}' not found in data.\")\n",
    "\n",
    "# Prepare data\n",
    "X_train, X_test, y_train, y_test, cat_cols, num_cols = prepare_data_for_modeling(df, target_col)\n",
    "print(f\"Data split into train ({len(X_train)} rows) and test ({len(X_test)} rows) sets\")\n",
    "print(f\"Categorical columns ({len(cat_cols)}): {cat_cols}\")\n",
    "print(f\"Numerical columns ({len(num_cols)}): {num_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Selection\n",
    "\n",
    "Let's select the most important features for modeling if we haven't already done so in the preprocessing notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature selection if we don't have selected features from preprocessing\n",
    "if selected_features is None:\n",
    "    max_features = 20\n",
    "    print(f\"Applying feature selection to reduce features (max: {max_features})\")\n",
    "    \n",
    "    # Compare different feature selection methods\n",
    "    feature_selection_results = compare_feature_selection_methods(X_train, y_train, k=max_features)\n",
    "    \n",
    "    # Display the features selected by each method\n",
    "    for method, features in feature_selection_results.items():\n",
    "        print(f\"{method} ({len(features)} features): {features}\")\n",
    "    \n",
    "    # Get common features selected by at least 2 methods\n",
    "    selected_features = get_common_features(feature_selection_results, min_methods=2)\n",
    "    print(f\"\\nCommon features selected by at least 2 methods ({len(selected_features)}):\\n{selected_features}\")\n",
    "    \n",
    "    # If we have too many features, use mutual information to select top max_features\n",
    "    if len(selected_features) > max_features:\n",
    "        print(f\"Selected {len(selected_features)} features, reducing to {max_features} using mutual information\")\n",
    "        _, selected_features = select_features_mutual_info(X_train, y_train, k=max_features)\n",
    "    \n",
    "    # If we have too few features, use mutual information to select features\n",
    "    if len(selected_features) < 5:\n",
    "        print(f\"Only {len(selected_features)} features selected, using mutual information to select {max_features}\")\n",
    "        _, selected_features = select_features_mutual_info(X_train, y_train, k=max_features)\n",
    "\n",
    "# Filter X_train and X_test to include only selected features\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n",
    "\n",
    "# Update categorical and numerical columns\n",
    "cat_cols_selected = [col for col in cat_cols if col in selected_features]\n",
    "num_cols_selected = [col for col in num_cols if col in selected_features]\n",
    "\n",
    "print(f\"\\nFinal selected features ({len(selected_features)}):\\n{selected_features}\")\n",
    "print(f\"\\nSelected categorical columns ({len(cat_cols_selected)}): {cat_cols_selected}\")\n",
    "print(f\"Selected numerical columns ({len(num_cols_selected)}): {num_cols_selected}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Baseline Model\n",
    "\n",
    "Let's train a baseline model to establish a performance benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a baseline model (mean prediction)\n",
    "print(\"Training Baseline model (mean prediction)\")\n",
    "baseline_model, baseline_metrics = train_baseline_model(X_train_selected, y_train, cat_cols_selected, num_cols_selected, strategy='mean')\n",
    "baseline_test_metrics = evaluate_model(baseline_model, X_test_selected, y_test)\n",
    "baseline_metrics.update(baseline_test_metrics)\n",
    "\n",
    "# Display baseline metrics\n",
    "print(\"\\nBaseline Model Metrics:\")\n",
    "for metric, value in baseline_metrics.items():\n",
    "    print(f\"{metric}: {value}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_baseline = baseline_model.predict(X_test_selected)\n",
    "\n",
    "# Plot predicted vs. actual values\n",
    "plot_prediction_vs_actual(y_test, y_pred_baseline, title='Baseline Model: Predicted vs. Actual Trip Duration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train and Evaluate Multiple Models\n",
    "\n",
    "Let's train and evaluate multiple regression models to predict trip duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to train and evaluate a model\n",
    "@timer_decorator\n",
    "def train_and_evaluate_model(model_name, X_train, y_train, X_test, y_test, cat_cols, num_cols):\n",
    "    print(f\"Training {model_name} model...\")\n",
    "    \n",
    "    # Train the model based on the model name\n",
    "    if model_name == 'Linear Regression':\n",
    "        model, train_metrics = train_linear_regression(X_train, y_train, cat_cols, num_cols)\n",
    "    elif model_name == 'Ridge Regression':\n",
    "        model, train_metrics = train_ridge_regression(X_train, y_train, cat_cols, num_cols)\n",
    "    elif model_name == 'Random Forest':\n",
    "        model, train_metrics = train_random_forest(X_train, y_train, cat_cols, num_cols)\n",
    "    elif model_name == 'Gradient Boosting':\n",
    "        model, train_metrics = train_gradient_boosting(X_train, y_train, cat_cols, num_cols)\n",
    "    elif model_name == 'K-Nearest Neighbors':\n",
    "        model, train_metrics = train_knn_regressor(X_train, y_train, cat_cols, num_cols)\n",
    "    elif model_name == 'Support Vector Machine':\n",
    "        model, train_metrics = train_svm_regressor(X_train, y_train, cat_cols, num_cols)\n",
    "    elif model_name == 'Naive Bayes':\n",
    "        model, train_metrics = train_naive_bayes(X_train, y_train, cat_cols, num_cols)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    test_metrics = evaluate_model(model, X_test, y_test)\n",
    "    train_metrics.update(test_metrics)\n",
    "    \n",
    "    # Get feature importance if available\n",
    "    if model_name in ['Random Forest', 'Gradient Boosting']:\n",
    "        importance_df = get_feature_importance(model, cat_cols, num_cols)\n",
    "    else:\n",
    "        importance_df = pd.DataFrame({'feature': [], 'importance': []})\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    return model, train_metrics, importance_df, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models to train\n",
    "models_to_train = [\n",
    "    'Linear Regression',\n",
    "    'Ridge Regression',\n",
    "    'Random Forest',\n",
    "    'Gradient Boosting',\n",
    "    'K-Nearest Neighbors',\n",
    "    'Support Vector Machine',\n",
    "    'Naive Bayes'\n",
    "]\n",
    "\n",
    "# Train and evaluate each model\n",
    "trained_models = {}\n",
    "model_metrics = {}\n",
    "model_importances = {}\n",
    "model_predictions = {}\n",
    "\n",
    "# Add baseline model\n",
    "trained_models['Baseline (Mean)'] = baseline_model\n",
    "model_metrics['Baseline (Mean)'] = baseline_metrics\n",
    "model_predictions['Baseline (Mean)'] = y_pred_baseline\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model_name in models_to_train:\n",
    "    model, metrics, importance_df, y_pred = train_and_evaluate_model(\n",
    "        model_name, X_train_selected, y_train, X_test_selected, y_test, cat_cols_selected, num_cols_selected\n",
    "    )\n",
    "    \n",
    "    trained_models[model_name] = model\n",
    "    model_metrics[model_name] = metrics\n",
    "    model_importances[model_name] = importance_df\n",
    "    model_predictions[model_name] = y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare Model Performance\n",
    "\n",
    "Let's compare the performance of all the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models based on test RMSE\n",
    "compare_models(model_metrics, metric='test_rmse', title='Model Comparison: Test RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models based on test R²\n",
    "compare_models(model_metrics, metric='test_r2', title='Model Comparison: Test R²')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary table of model metrics\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Model': list(model_metrics.keys()),\n",
    "    'Train RMSE': [metrics['train_rmse'] for metrics in model_metrics.values()],\n",
    "    'Test RMSE': [metrics['test_rmse'] for metrics in model_metrics.values()],\n",
    "    'Train R²': [metrics['train_r2'] for metrics in model_metrics.values()],\n",
    "    'Test R²': [metrics['test_r2'] for metrics in model_metrics.values()]\n",
    "})\n",
    "\n",
    "# Sort by test RMSE (ascending)\n",
    "metrics_df = metrics_df.sort_values('Test RMSE')\n",
    "\n",
    "# Display the summary table\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyze the Best Model\n",
    "\n",
    "Let's analyze the best-performing model in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the best model based on test RMSE\n",
    "best_model_name = metrics_df.iloc[0]['Model']\n",
    "best_model = trained_models[best_model_name]\n",
    "best_metrics = model_metrics[best_model_name]\n",
    "best_predictions = model_predictions[best_model_name]\n",
    "\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "print(\"\\nBest model metrics:\")\n",
    "for metric, value in best_metrics.items():\n",
    "    if isinstance(value, (int, float)):\n",
    "        print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted vs. actual values for the best model\n",
    "plot_prediction_vs_actual(y_test, best_predictions, title=f'{best_model_name}: Predicted vs. Actual Trip Duration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot residuals for the best model\n",
    "plot_residuals(y_test, best_predictions, title=f'{best_model_name}: Residual Analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot error distribution for the best model\n",
    "plot_error_distribution(y_test, best_predictions, title=f'{best_model_name}: Error Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance for the best model if available\n",
    "if best_model_name in ['Random Forest', 'Gradient Boosting']:\n",
    "    importance_df = model_importances[best_model_name]\n",
    "    plot_feature_importance(importance_df, title=f'{best_model_name}: Feature Importance', top_n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save the Best Model\n",
    "\n",
    "Let's save the best model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory for the best model\n",
    "best_model_dir = os.path.join(MODELS_DIR, taxi_type, 'best_model')\n",
    "os.makedirs(best_model_dir, exist_ok=True)\n",
    "\n",
    "# Save the best model\n",
    "best_model_path = os.path.join(best_model_dir, 'best_model.pkl')\n",
    "with open(best_model_path, 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "print(f\"Saved best model to {best_model_path}\")\n",
    "\n",
    "# Save the best model metrics\n",
    "best_metrics_path = os.path.join(best_model_dir, 'best_model_metrics.json')\n",
    "save_metrics(best_metrics, best_metrics_path)\n",
    "print(f\"Saved best model metrics to {best_metrics_path}\")\n",
    "\n",
    "# Save the selected features\n",
    "selected_features_path = os.path.join(best_model_dir, 'selected_features.txt')\n",
    "with open(selected_features_path, 'w') as f:\n",
    "    f.write('\\n'.join(selected_features))\n",
    "print(f\"Saved selected features to {selected_features_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Interpret the Results and Make Recommendations\n",
    "\n",
    "Let's interpret the results and make recommendations based on our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance if available\n",
    "if best_model_name in ['Random Forest', 'Gradient Boosting']:\n",
    "    importance_df = model_importances[best_model_name]\n",
    "    top_features = importance_df.head(10)\n",
    "    \n",
    "    print(f\"Top 10 most important features for {best_model_name}:\")\n",
    "    for i, (feature, importance) in enumerate(zip(top_features['feature'], top_features['importance'])):\n",
    "        print(f\"{i+1}. {feature}: {importance:.4f}\")\n",
    "else:\n",
    "    # For models without built-in feature importance, we can use permutation importance\n",
    "    from sklearn.inspection import permutation_importance\n",
    "    \n",
    "    print(f\"Computing permutation importance for {best_model_name}...\")\n",
    "    result = permutation_importance(best_model, X_test_selected, y_test, n_repeats=10, random_state=42)\n",
    "    perm_importance = pd.DataFrame({\n",
    "        'feature': X_test_selected.columns,\n",
    "        'importance': result.importances_mean\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    top_features = perm_importance.head(10)\n",
    "    print(f\"\\nTop 10 most important features for {best_model_name} (permutation importance):\")\n",
    "    for i, (feature, importance) in enumerate(zip(top_features['feature'], top_features['importance'])):\n",
    "        print(f\"{i+1}. {feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations Based on Feature Importance\n",
    "\n",
    "Based on the feature importance analysis, we can make the following recommendations:\n",
    "\n",
    "1. **Trip Distance**: Trip distance is one of the most important predictors of trip duration. This suggests that pricing strategies should be primarily based on distance.\n",
    "\n",
    "2. **Time of Day**: The hour of pickup is a significant factor. Trips during rush hours take longer, so surge pricing during these times is justified.\n",
    "\n",
    "3. **Day of Week**: Weekday vs. weekend patterns affect trip duration. Different pricing or resource allocation strategies might be needed for weekdays vs. weekends.\n",
    "\n",
    "4. **Location**: Pickup and dropoff locations significantly impact trip duration. Certain areas might need more taxis during specific times.\n",
    "\n",
    "5. **Speed**: Average speed is a key factor. Routes with consistently low speeds might need alternative paths or special pricing.\n",
    "\n",
    "6. **Payment Method**: If payment method is an important feature, this might indicate different customer behaviors based on how they pay.\n",
    "\n",
    "### Operational Recommendations:\n",
    "\n",
    "1. **Dynamic Pricing**: Implement more sophisticated dynamic pricing based on predicted trip duration, not just distance.\n",
    "\n",
    "2. **Resource Allocation**: Allocate more taxis to areas with high demand and longer predicted trip durations.\n",
    "\n",
    "3. **Route Optimization**: Identify and optimize routes with consistently longer-than-expected trip durations.\n",
    "\n",
    "4. **Customer Communication**: Provide more accurate trip duration estimates to customers based on the model's predictions.\n",
    "\n",
    "5. **Driver Training**: Train drivers on efficient routes for specific pickup/dropoff location pairs that tend to have longer durations.\n",
    "\n",
    "### Model Improvement Recommendations:\n",
    "\n",
    "1. **External Data**: Incorporate external data such as weather conditions, events, and traffic patterns.\n",
    "\n",
    "2. **Temporal Features**: Add more sophisticated temporal features like holidays, events, or seasonal patterns.\n",
    "\n",
    "3. **Geospatial Features**: Add more detailed geospatial features like distance to major landmarks or traffic hotspots.\n",
    "\n",
    "4. **Ensemble Methods**: Combine multiple models for better performance.\n",
    "\n",
    "5. **Real-time Updates**: Implement a system for real-time updates to the model based on current traffic conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "In this notebook, we have:\n",
    "\n",
    "1. Loaded the processed NYC taxi data\n",
    "2. Selected the most important features for modeling\n",
    "3. Trained a baseline model to establish a performance benchmark\n",
    "4. Trained and evaluated multiple regression models\n",
    "5. Compared the performance of all models\n",
    "6. Analyzed the best-performing model in detail\n",
    "7. Saved the best model for future use\n",
    "8. Interpreted the results and made recommendations\n",
    "\n",
    "The best-performing model was the [Best Model Name], which achieved a test RMSE of [Best RMSE] and a test R² of [Best R²]. This model can be used to predict trip durations for NYC taxi trips with reasonable accuracy.\n",
    "\n",
    "The most important features for predicting trip duration were [Top Features], which suggests that [Key Insights].\n",
    "\n",
    "Based on our analysis, we recommend [Key Recommendations]."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
